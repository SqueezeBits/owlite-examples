diff --git a/src/transformers/models/bert/modeling_bert.py b/src/transformers/models/bert/modeling_bert.py
index 29846b805..e3f1eac80 100755
--- a/src/transformers/models/bert/modeling_bert.py
+++ b/src/transformers/models/bert/modeling_bert.py
@@ -1516,6 +1516,33 @@ class BertForNextSentencePrediction(BertPreTrainedModel):
     """,
     BERT_START_DOCSTRING,
 )
+class BertForSequenceClassificationWrapper(torch.nn.Module):
+    def __init__(self, model, num_labels, loss_function):
+        super().__init__()
+        self.model = model
+        self.num_labels = num_labels
+        self.loss_function = loss_function
+
+    def forward(self, **kwargs):
+        if 'labels' in kwargs:
+            labels = kwargs.pop('labels')
+        lm_logits = self.model(**kwargs)
+
+        if isinstance(self.loss_function, torch.nn.MSELoss):
+            if self.num_labels == 1:
+                loss = self.loss_function(lm_logits.squeeze(), labels.squeeze())
+            else:
+                loss = self.loss_function(lm_logits, labels)
+        elif isinstance(self.loss_function, torch.nn.CrossEntropyLoss):
+            loss = self.loss_function(lm_logits.view(-1, self.num_labels), labels.view(-1))
+        elif isinstance(self.loss_function, torch.nn.BCEWithLogitsLoss):
+            loss = self.loss_function(lm_logits, labels)
+
+        return SequenceClassifierOutput(
+            loss=loss,
+            logits=lm_logits
+        )
+
 class BertForSequenceClassification(BertPreTrainedModel):
     def __init__(self, config):
         super().__init__(config)
@@ -1529,6 +1556,24 @@ class BertForSequenceClassification(BertPreTrainedModel):
         self.dropout = nn.Dropout(classifier_dropout)
         self.classifier = nn.Linear(config.hidden_size, config.num_labels)
 
+        # Define loss function depending on problem_type in config and num_labels
+        if self.config.problem_type is None:
+            raise ValueError(
+                f"The problem type ({config.problem_type}) should be defined."
+            )
+        if self.config.problem_type == "regression":
+            self.loss_fct = MSELoss()
+        elif self.config.problem_type == "single_label_classification":
+            self.loss_fct = CrossEntropyLoss()
+        elif self.config.problem_type == "multi_label_classification":
+            self.loss_fct = BCEWithLogitsLoss()
+        else:
+            raise ValueError(
+                f"The problem type ({config.problem_type}) is not defined"
+                "The problem type should be selected among regression" 
+                "single_label_classification, and multi_label_classification"
+            )
+        
         # Initialize weights and apply final processing
         self.post_init()
 
@@ -1580,29 +1625,17 @@ class BertForSequenceClassification(BertPreTrainedModel):
 
         loss = None
         if labels is not None:
-            if self.config.problem_type is None:
-                if self.num_labels == 1:
-                    self.config.problem_type = "regression"
-                elif self.num_labels > 1 and (labels.dtype == torch.long or labels.dtype == torch.int):
-                    self.config.problem_type = "single_label_classification"
-                else:
-                    self.config.problem_type = "multi_label_classification"
-
             if self.config.problem_type == "regression":
-                loss_fct = MSELoss()
                 if self.num_labels == 1:
-                    loss = loss_fct(logits.squeeze(), labels.squeeze())
+                    loss = self.loss_fct(logits.squeeze(), labels.squeeze())
                 else:
-                    loss = loss_fct(logits, labels)
+                    loss = self.loss_fct(logits, labels)
             elif self.config.problem_type == "single_label_classification":
-                loss_fct = CrossEntropyLoss()
-                loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))
+                loss = self.loss_fct(logits.view(-1, self.num_labels), labels.view(-1))
             elif self.config.problem_type == "multi_label_classification":
-                loss_fct = BCEWithLogitsLoss()
-                loss = loss_fct(logits, labels)
+                loss = self.loss_fct(logits, labels)
         if not return_dict:
-            output = (logits,) + outputs[2:]
-            return ((loss,) + output) if loss is not None else output
+            return logits
 
         return SequenceClassifierOutput(
             loss=loss,
