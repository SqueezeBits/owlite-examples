diff --git a/requirements.txt b/requirements.txt
index 80f6d17..cbae374 100644
--- a/requirements.txt
+++ b/requirements.txt
@@ -15,4 +15,4 @@ tensorboard
 # pycocotools corresponds to https://github.com/ppwwyyxx/cocoapi
 pycocotools>=2.0.2
 onnx>=1.13.0
-onnx-simplifier==0.4.10
+onnx-simplifier>=0.4.10
diff --git a/tools/eval.py b/tools/eval.py
index 83ad76b..f2df027 100644
--- a/tools/eval.py
+++ b/tools/eval.py
@@ -22,13 +22,60 @@ from yolox.utils import (
     get_model_info,
     setup_logger
 )
-
+from yolox.data import DataPrefetcher
+import owlite #import owlite
 
 def make_parser():
     parser = argparse.ArgumentParser("YOLOX Eval")
     parser.add_argument("-expn", "--experiment-name", type=str, default=None)
     parser.add_argument("-n", "--name", type=str, default=None, help="model name")
 
+    # Add OwLite arguments
+    subparsers = parser.add_subparsers(required=True)
+    owlite_parser = subparsers.add_parser("owlite", help="OwLite arguments")
+    owlite_parser.add_argument(
+        "--project",
+        type=str,
+        required=True,
+        dest="owlite_project",
+        help="OwLite project name"
+    )
+    owlite_parser.add_argument(
+        "--baseline",
+        type=str,
+        required=True,
+        dest="owlite_baseline",
+        help="OwLite baseline name"
+    )
+    owlite_parser.add_argument(
+        "--experiment",
+        type=str,
+        default=None,
+        dest="owlite_experiment",
+        help="OwLite experiment name"
+    )
+    owlite_parser.add_argument(
+        '--duplicate-from', 
+        type=str,
+        default=None,
+        dest="owlite_duplicate_from",
+        help="The name of OwLite experiment "
+            "where the config to be duplicated is located"
+    )
+    owlite_parser.add_argument(
+        '--ptq',
+        action="store_true",
+        dest="owlite_ptq",
+        help="True if OwLite PTQ is applied"
+    )
+    owlite_parser.add_argument(
+        '--calib-num',
+        type=int,
+        default=128,
+        dest="owlite_calib_num",
+        help="Number of data to use for OwLite calibration"
+    )
+
     # distributed
     parser.add_argument(
         "--dist-backend", default="nccl", type=str, help="distributed backend"
@@ -188,14 +235,51 @@ def main(exp, args, num_gpu):
     else:
         trt_file = None
         decoder = None
-
+ 
+    # init OwLite
+    owl = owlite.init(
+        project=args.owlite_project,
+        baseline=args.owlite_baseline,
+        experiment=args.owlite_experiment,
+        duplicate_from=args.owlite_duplicate_from
+    )
+    model.eval()
+    model.head.decode_in_inference = False
+    strides = model.head.strides
+    # run OwLite convert
+    with torch.no_grad():
+        dummy_input = torch.randn(args.batch_size, 3, *exp.input_size)
+        model = owl.convert(model, dummy_input)
+ 
+    if args.owlite_ptq:
+        # generate train dataloader for PTQ calibration
+        train_loader = exp.get_data_loader(
+            batch_size=args.batch_size,
+            is_distributed=False,
+            no_aug=True,
+            cache_img=None
+        )
+        prefetcher = DataPrefetcher(train_loader)
+        # calibrate the model with OwLite
+        with owlite.calibrate(model) as calibrate_model:
+            for idx in range(args.owlite_calib_num // args.batch_size):
+                inps, _  = prefetcher.next()
+                calibrate_model(inps)
+                if (idx + 1) * args.batch_size >= args.owlite_calib_num:
+                    break
+    
+    # benchmark the model using OwLite
+    owl.export(model)
+    owl.benchmark()
     # start evaluate
-    *_, summary = evaluator.evaluate(
-        model, is_distributed, args.fp16, trt_file, decoder, exp.test_size
+    ap50_95, _, summary = evaluator.evaluate(
+        model, is_distributed, args.fp16, trt_file, decoder, exp.test_size,
+        is_decoding_in_model = False, strides=strides,
     )
+    # log mAP to OwLite
+    owl.log(mAP=ap50_95)
     logger.info("\n" + summary)
 
-
 if __name__ == "__main__":
     configure_module()
     args = make_parser().parse_args()
diff --git a/tools/train.py b/tools/train.py
index d57f420..7fff688 100644
--- a/tools/train.py
+++ b/tools/train.py
@@ -19,6 +19,53 @@ def make_parser():
     parser = argparse.ArgumentParser("YOLOX train parser")
     parser.add_argument("-expn", "--experiment-name", type=str, default=None)
     parser.add_argument("-n", "--name", type=str, default=None, help="model name")
+    
+    
+    # Add OwLite arguments
+    subparsers = parser.add_subparsers(required=True)
+    owlite_parser = subparsers.add_parser("owlite", help="OwLite arguments")
+    owlite_parser.add_argument(
+        "--project",
+        type=str,
+        required=True,
+        dest="owlite_project",
+        help="OwLite project name"
+    )
+    owlite_parser.add_argument(
+        "--baseline",
+        type=str,
+        required=True,
+        dest="owlite_baseline",
+        help="OwLite baseline name"
+    )
+    owlite_parser.add_argument(
+        "--experiment",
+        type=str,
+        required=True,
+        dest="owlite_experiment",
+        help="OwLite experiment name"
+    )
+    owlite_parser.add_argument(
+        '--duplicate-from', 
+        type=str,
+        default=None,
+        dest="owlite_duplicate_from",
+        help="The name of OwLite experiment "
+            "where the config to be duplicated is located"
+    )
+    owlite_parser.add_argument(
+        '--qat',
+        action="store_true",
+        dest="owlite_qat",
+        help="True if OwLite QAT is applied"
+    )
+    owlite_parser.add_argument(
+        '--calib-num',
+        type=int,
+        default=128,
+        dest="owlite_calib_num",
+        help="Number of data to use for OwLite calibration"
+    )
 
     # distributed
     parser.add_argument(
@@ -48,7 +95,7 @@ def make_parser():
     parser.add_argument(
         "-e",
         "--start_epoch",
-        default=None,
+        default=295,
         type=int,
         help="resume training start epoch",
     )
diff --git a/yolox/core/trainer.py b/yolox/core/trainer.py
index a764426..09d1ebd 100644
--- a/yolox/core/trainer.py
+++ b/yolox/core/trainer.py
@@ -32,6 +32,9 @@ from yolox.utils import (
     synchronize
 )
 
+import owlite #import owlite
+from yolox.models.yolox import YOLOX
+from yolox.models.yolo_head import YOLOXHead
 
 class Trainer:
     def __init__(self, exp: Exp, args):
@@ -170,8 +173,43 @@ class Trainer:
             self.ema_model = ModelEMA(model, 0.9998)
             self.ema_model.updates = self.max_iter * self.start_epoch
 
-        self.model = model
-
+        if self.args.owlite_qat:
+            # init OwLite
+            self.owl = owlite.init(
+                project=self.args.owlite_project,
+                baseline=self.args.owlite_baseline,
+                experiment=self.args.owlite_experiment,
+                duplicate_from=self.args.owlite_duplicate_from
+            )
+            model.eval()
+            model.head.decode_in_inference = False
+            self.strides = model.head.strides
+            output_shapes = [
+                (self.exp.input_size[0] // stride,
+                self.exp.input_size[1] // stride) for stride in self.strides
+            ]
+            head = YOLOXHead(model.head.num_classes,
+                    strides=self.strides,
+                    owlite_qat=True,
+                    output_shapes=output_shapes)
+            
+            # run OwLite convert
+            with torch.no_grad():
+                dummy_input = torch.randn(self.args.batch_size, 3, *self.exp.input_size)
+                model = self.owl.convert(model, dummy_input)
+
+            # calibrate the model with OwLite
+            with owlite.calibrate(model) as calibrate_model:
+                for idx in range(self.args.owlite_calib_num // self.args.batch_size):
+                    inps, _  = self.prefetcher.next()
+                    calibrate_model(inps)
+                    if (idx + 1) * self.args.batch_size >= self.args.owlite_calib_num:
+                        break
+
+            self.model = YOLOX(model, head, owlite_qat = True)
+            self.model.train()
+        else:
+            self.model = model
         self.evaluator = self.exp.get_evaluator(
             batch_size=self.args.batch_size, is_distributed=self.is_distributed
         )
@@ -189,12 +227,17 @@ class Trainer:
                 raise ValueError("logger must be either 'tensorboard' or 'wandb'")
 
         logger.info("Training start...")
-        logger.info("\n{}".format(model))
 
     def after_train(self):
         logger.info(
             "Training of experiment is done and the best AP is {:.2f}".format(self.best_ap * 100)
         )
+        if self.args.owlite_qat:
+            self.model.eval()
+            # benchmark inference model
+            self.owl.export(self.model.backbone)
+            self.owl.benchmark()
+            self.owl.log(mAP=self.best_ap)
         if self.rank == 0:
             if self.args.logger == "wandb":
                 self.wandb_logger.finish()
@@ -218,7 +261,8 @@ class Trainer:
         self.save_ckpt(ckpt_name="latest")
 
         if (self.epoch + 1) % self.exp.eval_interval == 0:
-            all_reduce_norm(self.model)
+            if not self.args.owlite_qat:
+                all_reduce_norm(self.model)
             self.evaluate_and_save_model()
 
     def before_iter(self):
@@ -332,10 +376,17 @@ class Trainer:
             if is_parallel(evalmodel):
                 evalmodel = evalmodel.module
 
-        with adjust_status(evalmodel, training=False):
-            (ap50_95, ap50, summary), predictions = self.exp.eval(
-                evalmodel, self.evaluator, self.is_distributed, return_outputs=True
-            )
+        if self.args.owlite_qat:
+            with adjust_status(evalmodel, training=False):
+                (ap50_95, ap50, summary), predictions = self.evaluator.evaluate(
+                    evalmodel.backbone, self.is_distributed, return_outputs=True,
+                    is_decoding_in_model = False, strides=self.strides,
+                )
+        else:
+            with adjust_status(evalmodel, training=False):
+                (ap50_95, ap50, summary), predictions = self.exp.eval(
+                    evalmodel, self.evaluator, self.is_distributed, return_outputs=True
+                )
 
         update_best_ckpt = ap50_95 > self.best_ap
         self.best_ap = max(self.best_ap, ap50_95)
diff --git a/yolox/evaluators/coco_evaluator.py b/yolox/evaluators/coco_evaluator.py
index e218c74..b4914e0 100644
--- a/yolox/evaluators/coco_evaluator.py
+++ b/yolox/evaluators/coco_evaluator.py
@@ -27,6 +27,22 @@ from yolox.utils import (
     xyxy2xywh
 )
 
+def decode_outputs(outputs, hw, stride_info):
+    grids = []
+    strides = []
+    for (hsize, wsize), stride in zip(hw, stride_info):
+        yv, xv = torch.meshgrid([torch.arange(hsize), torch.arange(wsize)])
+        grid = torch.stack((xv, yv), 2).view(1, -1, 2)
+        grids.append(grid)
+        shape = grid.shape[:2]
+        strides.append(torch.full((*shape, 1), stride))
+
+    grids = torch.cat(grids, dim=1).type(outputs.type())
+    strides = torch.cat(strides, dim=1).type(outputs.type())
+
+    outputs[..., :2] = (outputs[..., :2] + grids) * strides
+    outputs[..., 2:4] = torch.exp(outputs[..., 2:4]) * strides
+    return outputs
 
 def per_class_AR_table(coco_eval, class_names=COCO_CLASSES, headers=["class", "AR"], colums=6):
     per_class_AR = {}
@@ -115,7 +131,7 @@ class COCOEvaluator:
 
     def evaluate(
         self, model, distributed=False, half=False, trt_file=None,
-        decoder=None, test_size=None, return_outputs=False
+        decoder=None, test_size=None, return_outputs=False, is_decoding_in_model=True, strides=None
     ):
         """
         COCO average precision (AP) Evaluation. Iterate inference on the test dataset
@@ -167,6 +183,12 @@ class COCOEvaluator:
                     start = time.time()
 
                 outputs = model(imgs)
+
+                # if not is_decoding_in_model, decode outputs
+                if not is_decoding_in_model:
+                    hw = [(imgs.shape[2] // s, imgs.shape[3] // s) for s in strides]
+                    outputs = decode_outputs(outputs, hw, tuple(strides))
+
                 if decoder is not None:
                     outputs = decoder(outputs, dtype=outputs.type())
 
diff --git a/yolox/models/yolo_head.py b/yolox/models/yolo_head.py
index 3e51768..51f835b 100644
--- a/yolox/models/yolo_head.py
+++ b/yolox/models/yolo_head.py
@@ -13,7 +13,7 @@ from yolox.utils import bboxes_iou, cxcywh2xyxy, meshgrid, visualize_assign
 
 from .losses import IOUloss
 from .network_blocks import BaseConv, DWConv
-
+from typing import Optional, List, Tuple
 
 class YOLOXHead(nn.Module):
     def __init__(
@@ -24,102 +24,114 @@ class YOLOXHead(nn.Module):
         in_channels=[256, 512, 1024],
         act="silu",
         depthwise=False,
+        owlite_qat=False,
+        output_shapes:Optional[List[Tuple[int]]] = None,
     ):
         """
         Args:
             act (str): activation type of conv. Defalut value: "silu".
             depthwise (bool): whether apply depthwise conv in conv branch. Defalut value: False.
+            owlite_qat (bool): whether the module is for OwLite QAT.
+                if True, the head module will include only loss-related modules.
+                Default value: False.
+            output_shapes (Optional[List[Tuple[int]]]): List of output shapes of each YOLOX head.
+                if owlite_qat is True, this argument is required.
+                Default value: None.
         """
         super().__init__()
 
         self.num_classes = num_classes
         self.decode_in_inference = True  # for deploy, set to False
-
-        self.cls_convs = nn.ModuleList()
-        self.reg_convs = nn.ModuleList()
-        self.cls_preds = nn.ModuleList()
-        self.reg_preds = nn.ModuleList()
-        self.obj_preds = nn.ModuleList()
-        self.stems = nn.ModuleList()
-        Conv = DWConv if depthwise else BaseConv
-
-        for i in range(len(in_channels)):
-            self.stems.append(
-                BaseConv(
-                    in_channels=int(in_channels[i] * width),
-                    out_channels=int(256 * width),
-                    ksize=1,
-                    stride=1,
-                    act=act,
+        if not owlite_qat:
+            self.cls_convs = nn.ModuleList()
+            self.reg_convs = nn.ModuleList()
+            self.cls_preds = nn.ModuleList()
+            self.reg_preds = nn.ModuleList()
+            self.obj_preds = nn.ModuleList()
+            self.stems = nn.ModuleList()
+            Conv = DWConv if depthwise else BaseConv
+
+            for i in range(len(in_channels)):
+                self.stems.append(
+                    BaseConv(
+                        in_channels=int(in_channels[i] * width),
+                        out_channels=int(256 * width),
+                        ksize=1,
+                        stride=1,
+                        act=act,
+                    )
                 )
-            )
-            self.cls_convs.append(
-                nn.Sequential(
-                    *[
-                        Conv(
-                            in_channels=int(256 * width),
-                            out_channels=int(256 * width),
-                            ksize=3,
-                            stride=1,
-                            act=act,
-                        ),
-                        Conv(
-                            in_channels=int(256 * width),
-                            out_channels=int(256 * width),
-                            ksize=3,
-                            stride=1,
-                            act=act,
-                        ),
-                    ]
+                self.cls_convs.append(
+                    nn.Sequential(
+                        *[
+                            Conv(
+                                in_channels=int(256 * width),
+                                out_channels=int(256 * width),
+                                ksize=3,
+                                stride=1,
+                                act=act,
+                            ),
+                            Conv(
+                                in_channels=int(256 * width),
+                                out_channels=int(256 * width),
+                                ksize=3,
+                                stride=1,
+                                act=act,
+                            ),
+                        ]
+                    )
                 )
-            )
-            self.reg_convs.append(
-                nn.Sequential(
-                    *[
-                        Conv(
-                            in_channels=int(256 * width),
-                            out_channels=int(256 * width),
-                            ksize=3,
-                            stride=1,
-                            act=act,
-                        ),
-                        Conv(
-                            in_channels=int(256 * width),
-                            out_channels=int(256 * width),
-                            ksize=3,
-                            stride=1,
-                            act=act,
-                        ),
-                    ]
+                self.reg_convs.append(
+                    nn.Sequential(
+                        *[
+                            Conv(
+                                in_channels=int(256 * width),
+                                out_channels=int(256 * width),
+                                ksize=3,
+                                stride=1,
+                                act=act,
+                            ),
+                            Conv(
+                                in_channels=int(256 * width),
+                                out_channels=int(256 * width),
+                                ksize=3,
+                                stride=1,
+                                act=act,
+                            ),
+                        ]
+                    )
                 )
-            )
-            self.cls_preds.append(
-                nn.Conv2d(
-                    in_channels=int(256 * width),
-                    out_channels=self.num_classes,
-                    kernel_size=1,
-                    stride=1,
-                    padding=0,
+                self.cls_preds.append(
+                    nn.Conv2d(
+                        in_channels=int(256 * width),
+                        out_channels=self.num_classes,
+                        kernel_size=1,
+                        stride=1,
+                        padding=0,
+                    )
                 )
-            )
-            self.reg_preds.append(
-                nn.Conv2d(
-                    in_channels=int(256 * width),
-                    out_channels=4,
-                    kernel_size=1,
-                    stride=1,
-                    padding=0,
+                self.reg_preds.append(
+                    nn.Conv2d(
+                        in_channels=int(256 * width),
+                        out_channels=4,
+                        kernel_size=1,
+                        stride=1,
+                        padding=0,
+                    )
                 )
-            )
-            self.obj_preds.append(
-                nn.Conv2d(
-                    in_channels=int(256 * width),
-                    out_channels=1,
-                    kernel_size=1,
-                    stride=1,
-                    padding=0,
+                self.obj_preds.append(
+                    nn.Conv2d(
+                        in_channels=int(256 * width),
+                        out_channels=1,
+                        kernel_size=1,
+                        stride=1,
+                        padding=0,
+                    )
                 )
-            )
+        else:
+            self.output_shapes = output_shapes
+            self.last_channels_per_head = [shape[0] * shape[1] for shape in self.output_shapes]
+            self.forward = self.forward_for_owlite_qat
 
         self.use_l1 = False
         self.l1_loss = nn.L1Loss(reduction="none")
@@ -202,13 +214,13 @@ class YOLOXHead(nn.Module):
                 dtype=xin[0].dtype,
             )
         else:
-            self.hw = [x.shape[-2:] for x in outputs]
+            hw = [x.shape[-2:] for x in outputs]
             # [batch, n_anchors_all, 85]
             outputs = torch.cat(
                 [x.flatten(start_dim=2) for x in outputs], dim=2
             ).permute(0, 2, 1)
             if self.decode_in_inference:
-                return self.decode_outputs(outputs, dtype=xin[0].type())
+                return self.decode_outputs(outputs, hw, dtype=xin[0].type())
             else:
                 return outputs
 
@@ -232,10 +244,10 @@ class YOLOXHead(nn.Module):
         output[..., 2:4] = torch.exp(output[..., 2:4]) * stride
         return output, grid
 
-    def decode_outputs(self, outputs, dtype):
+    def decode_outputs(self, outputs, hw, dtype):
         grids = []
         strides = []
-        for (hsize, wsize), stride in zip(self.hw, self.strides):
+        for (hsize, wsize), stride in zip(hw, self.strides):
             yv, xv = meshgrid([torch.arange(hsize), torch.arange(wsize)])
             grid = torch.stack((xv, yv), 2).view(1, -1, 2)
             grids.append(grid)
@@ -639,3 +651,52 @@ class YOLOXHead(nn.Module):
             save_name = save_prefix + str(batch_idx) + ".png"
             img = visualize_assign(img, xyxy_boxes, coords, matched_gt_inds, save_name)
             logger.info(f"save img to {save_name}")
+
+    def forward_for_owlite_qat(self, input, labels=None, imgs=None):
+        outputs = []
+        origin_preds = []
+        x_shifts = []
+        y_shifts = []
+        expanded_strides = []
+        
+        # split input into reg_output, obj_output, and cls_output of each head.
+        input = input.permute(0, 2, 1)
+        head_outputs = torch.split(input, self.last_channels_per_head, dim=-1)
+        
+        for idx, output in enumerate(head_outputs):
+            output = output.reshape((output.shape[0], output.shape[1], 
+                                     self.output_shapes[idx][0],self.output_shapes[idx][1]))
+            
+            reg_output, obj_output, cls_output = torch.split(output, [4, 1, self.num_classes], dim=1)
+            
+            obj_output, cls_output = torch.special.logit(obj_output), torch.special.logit(cls_output)
+            
+            output = torch.cat([reg_output, obj_output, cls_output], 1)
+            output, grid = self.get_output_and_grid(
+                output, idx, self.strides[idx], input.type()
+            )
+
+            x_shifts.append(grid[:, :, 0])
+            y_shifts.append(grid[:, :, 1])
+            expanded_strides.append(
+                torch.zeros(1, grid.shape[1])
+                .fill_(self.strides[idx])
+                .type_as(input)
+            )
+            if self.use_l1:
+                reg_output = reg_output.permute(0, 2, 3, 1).reshape(
+                    reg_output.shape[0], -1, 4
+                )
+                origin_preds.append(reg_output.clone())
+            outputs.append(output)
+
+        return self.get_losses(
+            None,
+            x_shifts,
+            y_shifts,
+            expanded_strides,
+            labels,
+            torch.cat(outputs, 1),
+            origin_preds,
+            dtype=input.dtype,
+        )
\ No newline at end of file
diff --git a/yolox/models/yolo_pafpn.py b/yolox/models/yolo_pafpn.py
index 4c4e18a..6ddb464 100644
--- a/yolox/models/yolo_pafpn.py
+++ b/yolox/models/yolo_pafpn.py
@@ -7,7 +7,7 @@ import torch.nn as nn
 
 from .darknet import CSPDarknet
 from .network_blocks import BaseConv, CSPLayer, DWConv
-
+import torch.nn.functional as F
 
 class YOLOPAFPN(nn.Module):
     """
@@ -29,7 +29,6 @@ class YOLOPAFPN(nn.Module):
         self.in_channels = in_channels
         Conv = DWConv if depthwise else BaseConv
 
-        self.upsample = nn.Upsample(scale_factor=2, mode="nearest")
         self.lateral_conv0 = BaseConv(
             int(in_channels[2] * width), int(in_channels[1] * width), 1, 1, act=act
         )
@@ -95,12 +94,12 @@ class YOLOPAFPN(nn.Module):
         [x2, x1, x0] = features
 
         fpn_out0 = self.lateral_conv0(x0)  # 1024->512/32
-        f_out0 = self.upsample(fpn_out0)  # 512/16
+        f_out0 = F.interpolate(fpn_out0, (fpn_out0.shape[2]*2, fpn_out0.shape[3]*2))  # 512/16
         f_out0 = torch.cat([f_out0, x1], 1)  # 512->1024/16
         f_out0 = self.C3_p4(f_out0)  # 1024->512/16
 
         fpn_out1 = self.reduce_conv1(f_out0)  # 512->256/16
-        f_out1 = self.upsample(fpn_out1)  # 256/8
+        f_out1 = F.interpolate(fpn_out1, (fpn_out1.shape[2]*2, fpn_out1.shape[3]*2))  # 256/8
         f_out1 = torch.cat([f_out1, x2], 1)  # 256->512/8
         pan_out2 = self.C3_p3(f_out1)  # 512->256/8
 
diff --git a/yolox/models/yolox.py b/yolox/models/yolox.py
index 744ceea..4606ac3 100644
--- a/yolox/models/yolox.py
+++ b/yolox/models/yolox.py
@@ -13,9 +13,12 @@ class YOLOX(nn.Module):
     YOLOX model module. The module list is defined by create_yolov3_modules function.
     The network returns loss values from three YOLO layers during training
     and detection results during test.
+    If owlite_qat is True, `backbone` requires a converted YOLOX model including 
+    `YOLOPAFPN` backbone and `YOLOXHead` head with `owl.convert` function,  
+    and `head` requires a `YOLOXHead` module initiated with `owlite_qat=True`. 
     """
 
-    def __init__(self, backbone=None, head=None):
+    def __init__(self, backbone=None, head=None, owlite_qat=False):
         super().__init__()
         if backbone is None:
             backbone = YOLOPAFPN()
@@ -24,12 +27,13 @@ class YOLOX(nn.Module):
 
         self.backbone = backbone
         self.head = head
+        self.owlite_qat = owlite_qat
 
     def forward(self, x, targets=None):
         # fpn output content features of [dark3, dark4, dark5]
         fpn_outs = self.backbone(x)
 
-        if self.training:
+        if self.training or self.owlite_qat:
             assert targets is not None
             loss, iou_loss, conf_loss, cls_loss, l1_loss, num_fg = self.head(
                 fpn_outs, targets, x
diff --git a/yolox/utils/lr_scheduler.py b/yolox/utils/lr_scheduler.py
index 42c00cf..f9eedd8 100644
--- a/yolox/utils/lr_scheduler.py
+++ b/yolox/utils/lr_scheduler.py
@@ -89,6 +89,8 @@ class LRScheduler:
             ]
             gamma = getattr(self, "gamma", 0.1)
             lr_func = partial(multistep_lr, self.lr, milestones, gamma)
+        elif name == "constant": # constant lr schedule
+            lr_func = partial(constant_lr, self.lr)
         else:
             raise ValueError("Scheduler version {} not supported.".format(name))
         return lr_func
@@ -203,3 +205,7 @@ def multistep_lr(lr, milestones, gamma, iters):
     for milestone in milestones:
         lr *= gamma if iters >= milestone else 1.0
     return lr
+
+def constant_lr(lr, iters):
+    """Constant learning rate"""
+    return lr
\ No newline at end of file
